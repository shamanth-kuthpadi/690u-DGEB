# -*- coding: utf-8 -*-
"""DGEB_KMeans_Embeddings

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sg0bnRty4bNlyZWFIQycxvRdKEnq_zLC

# Setup
"""

#MOUNT CONTENT FROM DRIVE
from google.colab import drive
drive.mount('/content/drive/')

#declare runtime CPU or GPU
import torch
def getDevice():
    if torch.cuda.is_available():
        device = torch.device('cuda', 0)
    else:
        device = 'cpu'
    print(f"training with {device}")
    return device
getDevice()

!pip install dgeb
!pip install datasets
!pip install pytrec_eval

import datasets
import dgeb
import pytrec_eval
import numpy as np

"""# Run K-Means on Existing Embedding Approach"""

# Get task
tasks = dgeb.get_tasks_by_name(["ecoli_rna_clustering"])

# Load pretrained model
model = dgeb.get_model("facebook/esm2_t6_8M_UR50D")

# Evaluate
evaluation = dgeb.DGEB(tasks=tasks)
print("evaluating result")
results = evaluation.run(model, output_folder=None)  # avoid writing files
task_result = results[0]

for layer_result in task_result.results:
    print(f"Layer {layer_result.layer_number}:")
    for metric in layer_result.metrics:
        print(f"  {metric.display_name}: {metric.value}")

"""# Tokenize Data"""

#Given a list of DNA or protein sequences, tokenize into vectors
def get_tokenizations(sequences: list[str], max_length=None) -> torch.Tensor:
    sequence_to_vector = {'A': [1,0,0,0], 'C': [0,1,0,0], 'T': [0,0,1,0], 'G': [0,0,0,1], 'PAD': [0,0,0,0]}

    if max_length is None:
        max_length = max(len(seq) for seq in sequences)

    resulting_tokenization = []
    for sequence in sequences:
        tokenization = [sequence_to_vector[elem] for elem in sequence]

        # Pad with 'PAD' vectors if sequence is shorter than max_length
        while len(tokenization) < max_length:
            tokenization.append(sequence_to_vector['PAD'])

        # Truncate if longer
        tokenization = tokenization[:max_length]

        resulting_tokenization.append(tokenization)

    return torch.tensor(resulting_tokenization, dtype=torch.float32)  # shape: [N, G, M]
print(get_tokenizations(["ACTG", "AAAA", "CTGA"]).shape)

"""# Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding Algorithm Implementation

"""

class FourierPositionSequences():
  def __init__(self, sequences: list[str], F, H):
        self.sequences = sequences  # Instance attribute
        self.F = F
        self.H = H
  def gelu(self, x):
    return 0.5*x * (1 + np.tanh( np.sqrt(2 / np.pi)*(x + 0.044715*x**3)     ))
  def eq_2(self, x, W):
    D = len(x)
    return (1/np.sqrt(D))*(np.concatenate( [np.cos(x @ W.T)    ,  np.sin(x @ W.T)]    , axis = -1))
  def two_layer_NN(self, F, W_1, W_2, B_1, B_2):
    return (self.gelu(F @ W_1 + B_1)) @  W_2 + B_2
  def get_fourier_positional_sequences(self):
    x = get_tokenizations(self.sequences)
    N,G,M = x.shape
    W = torch.randn(self.F//2, M)

    W_1 = np.random.randn(self.F, self.H)  # [F, H]
    B_1 = np.random.randn(self.H)              # [H]
    D_out = G * 16  # e.g., target output dimension D = 16 * G
    W_2 = np.random.randn(self.H, D_out // G)  # [H, D/G]
    B_2 = np.random.randn(D_out // G)              # [D/G]



    F = (1 / np.sqrt(self.F)) * self.eq_2(x, W)

    #initialize neural network here with two layers and a GeLU activation function

    Y = self.two_layer_NN(F, W_1, W_2, B_1, B_2)
    return np.reshape(Y, (N, D_out))

import torch
import torch.nn as nn
import torch.nn.functional as F

class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = F.gelu(x)
        x = self.fc2(x)
        return x

"""# Evaluate K-means clustering on embeddings"""

from datasets import load_dataset
from sklearn.cluster import MiniBatchKMeans

from sklearn.metrics import v_measure_score

dataset = load_dataset("tattabio/e_coli_rnas")  # returns a Hugging Face dataset
print(dataset)


class ClusteringEvaluator():
    def __init__(
        self,
        embeds,
        labels,
        clustering_batch_size=500,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.embeds = embeds
        self.labels = labels
        self.clustering_batch_size = clustering_batch_size

    def __call__(self):
        corpus_embeddings = np.asarray(self.embeds)

        clustering_model = MiniBatchKMeans(
            n_clusters=len(set(self.labels)),
            batch_size=self.clustering_batch_size,
            n_init="auto",
        )
        clustering_model.fit(corpus_embeddings)
        cluster_assignment = clustering_model.labels_

        v_measure = v_measure_score(self.labels, cluster_assignment)

        return {"v_measure": v_measure}

sequences = dataset["train"]["Sequence"]
labels = dataset["train"]["Label"]

# Fourier embeddings
fourier_encoder = FourierPositionSequences(sequences=sequences, F=64, H=128)
embeddings = fourier_encoder.get_fourier_positional_sequences()

# Evaluate
evaluator = ClusteringEvaluator(embeds=embeddings, labels=labels)
results = evaluator()

print("V-measure:", results["v_measure"])